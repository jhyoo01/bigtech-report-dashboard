#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BigTech News Updater for GitHub Pages
ì‹¤ì œ ë¹…í…Œí¬ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  bigtech_data_latest.jsonì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import re
import json
import time
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from collections import defaultdict
from urllib.parse import quote_plus, urlparse
import hashlib


class BigTechNewsUpdater:
    """ë¹…í…Œí¬ ë‰´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸"""
    
    def __init__(self):
        self.news_data = {}
        self.all_news = []
        self.companies = {
            'Google': ['Google', 'Alphabet', 'Sundar Pichai'],
            'Meta': ['Meta', 'Facebook', 'Instagram', 'Mark Zuckerberg'],
            'YouTube': ['YouTube', 'YouTube Premium'],
            'TikTok': ['TikTok', 'ByteDance'],
            'PayPal': ['PayPal', 'Venmo'],
            'Stripe': ['Stripe', 'Patrick Collison'],
            'Microsoft': ['Microsoft', 'Satya Nadella', 'Azure'],
            'Amazon': ['Amazon', 'AWS', 'Andy Jassy']
        }
        
        for company in self.companies.keys():
            self.news_data[company] = []
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.target_sources = ['theverge.com', 'techcrunch.com']  # ì£¼ìš” ì¶œì²˜
        self.seen_hashes = set()
    
    def translate_to_korean(self, text):
        """Google Translate APIë¥¼ ì‚¬ìš©í•œ í•œê¸€ ë²ˆì—­"""
        if not text or len(text.strip()) == 0:
            return text
        
        try:
            # Google Translate ë¬´ë£Œ API ì‚¬ìš©
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'en',  # source language: English
                'tl': 'ko',  # target language: Korean
                'dt': 't',
                'q': text
            }
            
            response = requests.get(url, params=params, timeout=5)
            if response.status_code == 200:
                result = response.json()
                # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                translated = ''.join([item[0] for item in result[0] if item[0]])
                return translated
            else:
                return text
        except Exception as e:
            print(f"    ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text
    
    def is_duplicate(self, title):
        """ì¤‘ë³µ ì²´í¬"""
        title_hash = hashlib.md5(title.encode()).hexdigest()
        if title_hash in self.seen_hashes:
            return True
        self.seen_hashes.add(title_hash)
        return False
    
    def search_bigtech_news(self):
        """ì‹¤ì œ ë¹…í…Œí¬ ë‰´ìŠ¤ ê²€ìƒ‰"""
        print("ğŸ” ë¹…í…Œí¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        for company, keywords in self.companies.items():
            print(f"\nğŸ“± {company} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
            company_news = []
            
            for keyword in keywords[:2]:  # ì£¼ìš” í‚¤ì›Œë“œ 2ê°œë§Œ
                try:
                    print(f"  ê²€ìƒ‰ ì¤‘: {keyword}")
                    news = self.fetch_real_news(keyword, company, max_items=25)
                    company_news.extend(news)
                    time.sleep(0.5)
                except Exception as e:
                    print(f"âš ï¸  {keyword} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ì •ë ¬: ìš°ì„  ì¶œì²˜ > ìµœì‹ ìˆœ
            def sort_key(n):
                is_priority = any(s in n.get('url', '').lower() for s in self.target_sources)
                priority = 0 if is_priority else 1
                return (priority, n.get('published_date', '2000-01-01'))
            
            company_news.sort(key=sort_key, reverse=True)
            self.news_data[company] = company_news[:50]  # ìµœëŒ€ 50ê°œ
            
            print(f"  âœ… {len(self.news_data[company])}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ìš°ì„  ì¶œì²˜ í†µê³„
            priority_count = sum(1 for n in self.news_data[company] 
                               if any(s in n.get('url', '').lower() for s in self.target_sources))
            print(f"  ğŸ“° ìš°ì„  ì¶œì²˜: {priority_count}ê°œ")
    
    def fetch_real_news(self, keyword, company, max_items=25):
        """ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰ (Google News RSS í™œìš©)"""
        news_items = []
        
        try:
            # Google News RSS í”¼ë“œ ì‚¬ìš©
            rss_url = f"https://news.google.com/rss/search?q={quote_plus(keyword)}&hl=en-US&gl=US&ceid=US:en"
            
            response = requests.get(rss_url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return news_items
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')[:max_items * 2]
            
            for item in items:
                try:
                    title = item.title.text if item.title else ""
                    link = item.link.text if item.link else ""
                    pub_date = item.pubDate.text if item.pubDate else ""
                    description = item.description.text if item.description else ""
                    source_tag = item.source.text if item.source else "News"
                    
                    # ì¤‘ë³µ ì²´í¬
                    if self.is_duplicate(title):
                        continue
                    
                    # ë°œí–‰ ë‚ ì§œ íŒŒì‹± ë° 14ì¼ í•„í„°ë§
                    from email.utils import parsedate_to_datetime
                    try:
                        pub_datetime = parsedate_to_datetime(pub_date)
                        now = datetime.now(pub_datetime.tzinfo)
                        days_old = (now - pub_datetime).days
                        
                        # 14ì¼ ì´ìƒ ëœ ë‰´ìŠ¤ëŠ” ìŠ¤í‚µ
                        if days_old > 14:
                            continue
                        
                        actual_date = pub_datetime.strftime('%Y-%m-%d')
                    except:
                        actual_date = datetime.now().strftime('%Y-%m-%d')
                        days_old = 0
                    
                    # íšŒì‚¬ëª… ê´€ë ¨ì„± ì²´í¬
                    title_lower = title.lower()
                    if not any(kw.lower() in title_lower for kw in self.companies[company]):
                        continue
                    
                    # ì¶œì²˜ ë„ë©”ì¸ ì¶”ì¶œ
                    try:
                        domain = urlparse(link).netloc.replace('www.', '')
                        source = domain if domain else source_tag
                    except:
                        source = source_tag
                    
                    # ìš°ì„  ì¶œì²˜ í™•ì¸
                    is_target_source = any(target in link.lower() for target in self.target_sources)
                    
                    # ì„¤ëª… ì •ì œ
                    description_clean = BeautifulSoup(description, 'html.parser').get_text()
                    description_clean = description_clean[:200] + "..." if len(description_clean) > 200 else description_clean
                    
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    category = self.classify_category(title + " " + description_clean)
                    
                    # ì¤‘ìš”ë„ ê³„ì‚°
                    importance = self.calculate_importance(title, source, is_target_source)
                    
                    # í•œê¸€ ë²ˆì—­
                    print(f"    ë²ˆì—­ ì¤‘: {title[:50]}... ({days_old}ì¼ ì „)")
                    title_ko = self.translate_to_korean(title)
                    snippet_ko = self.translate_to_korean(description_clean) if description_clean else ""
                    
                    news_items.append({
                        'title': title,
                        'title_ko': title_ko,
                        'url': link,
                        'snippet': description_clean,
                        'snippet_ko': snippet_ko,
                        'source': source,
                        'published_date': actual_date,
                        'category': category,
                        'importance_score': importance,
                        'company': company
                    })
                    
                    if len(news_items) >= max_items:
                        break
                    
                    time.sleep(0.3)  # ë²ˆì—­ API rate limit ë°©ì§€
                    
                except Exception as e:
                    print(f"    í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            print(f"  RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        
        return news_items
    
    def classify_category(self, text):
        """ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['launch', 'release', 'unveil', 'announce']):
            return 'Product Launch'
        elif any(word in text_lower for word in ['revenue', 'earnings', 'profit', 'stock']):
            return 'Financial Results'
        elif any(word in text_lower for word in ['regulation', 'law', 'lawsuit', 'fine', 'court']):
            return 'Regulation & Policy'
        elif any(word in text_lower for word in ['ai', 'artificial intelligence', 'technology', 'innovation']):
            return 'Technology Innovation'
        elif any(word in text_lower for word in ['compete', 'rival', 'versus', 'market share']):
            return 'Competition'
        elif any(word in text_lower for word in ['ceo', 'executive', 'appoint', 'resign']):
            return 'Leadership & Strategy'
        else:
            return 'Market Trends'
    
    def calculate_importance(self, title, source, is_target_source=False):
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 50.0
        
        # ìš°ì„  ì¶œì²˜ ê°€ì¤‘ì¹˜
        if is_target_source:
            score += 25.0
        
        # ì œëª© í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        title_lower = title.lower()
        high_impact = ['billion', 'acquisition', 'launch', 'announces', 'breakthrough', 'major']
        for word in high_impact:
            if word in title_lower:
                score += 10.0
                break
        
        # ì¶œì²˜ ê°€ì¤‘ì¹˜
        premium_sources = ['TechCrunch', 'The Verge', 'Bloomberg', 'Reuters', 'CNBC']
        if any(s.lower() in source.lower() for s in premium_sources):
            score += 10.0
        
        return min(100.0, round(score, 1))
    
    def generate_json(self):
        """JSON ë°ì´í„° ìƒì„±"""
        print("\nğŸ“ JSON ìƒì„± ì¤‘...")
        
        stats = {}
        for company, articles in self.news_data.items():
            stats[f'{company}_count'] = len(articles)
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'statistics': stats,
            'news': self.news_data,
            'metadata': {
                'total_articles': sum(stats.values()),
                'companies': list(self.companies.keys()),
                'collection_date': datetime.now().strftime('%Y-%m-%d'),
                'cutoff_days': 14,
                'priority_sources': self.target_sources,
                'generated_by': 'BigTech News Crawler v1.0'
            }
        }
        
        return data
    
    def save_json(self, data):
    """JSON íŒŒì¼ ì €ì¥"""
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # data í´ë”ì— ì €ì¥
    filepath = 'data/bigtech_data_latest.json'
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {filepath} ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ BigTech News Updater ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
            self.search_bigtech_news()
            
            # 2. JSON ìƒì„±
            data = self.generate_json()
            
            # 3. ì €ì¥
            self.save_json(data)
            
            print("\n" + "=" * 60)
            print("âœ¨ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print(f"ğŸ“… ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“Š ì´ {data['metadata']['total_articles']}ê°œ ê¸°ì‚¬")
            
            # ê¸°ì—…ë³„ í†µê³„
            print("\nğŸ“Š ê¸°ì—…ë³„ í†µê³„:")
            for company, count in data['statistics'].items():
                print(f"  {company}: {count}")
            
            # ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
            print("\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°:")
            count = 0
            for company, articles in self.news_data.items():
                for article in articles[:2]:
                    count += 1
                    print(f"{count}. [{article['source']}] {article['title_ko'][:60]}...")
                    if count >= 5:
                        break
                if count >= 5:
                    break
            
            return 0
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return 1


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    updater = BigTechNewsUpdater()
    return updater.run()


if __name__ == "__main__":
    exit(main())
