#!/usr/bin/env python3
"""
ë¹…í…Œí¬ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ì™„ì „ ë¬´ë£Œ)
Google News RSSë¥¼ í™œìš©í•œ ë‰´ìŠ¤ í¬ë¡¤ë§
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import hashlib
import re
from urllib.parse import quote
import time

class BigTechNewsCrawler:
    """ë¬´ë£Œ ë¹…í…Œí¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    COMPANIES = {
        'Google': ['Google', 'Alphabet'],
        'Meta': ['Meta', 'Facebook', 'Instagram'],
        'YouTube': ['YouTube'],
        'TikTok': ['TikTok', 'ByteDance'],
        'PayPal': ['PayPal'],
        'Stripe': ['Stripe'],
        'Microsoft': ['Microsoft'],
        'Amazon': ['Amazon', 'AWS']
    }
    
    PRIORITY_SOURCES = ['techcrunch.com', 'theverge.com', 'TechCrunch', 'The Verge']
    
    def __init__(self):
        self.news_data = {}
        self.seen_titles = set()
        self.cutoff_date = datetime.now() - timedelta(days=14)
        
        for company in self.COMPANIES.keys():
            self.news_data[company] = []
    
    def generate_hash(self, title):
        """ì œëª© í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì œê±°)"""
        normalized = re.sub(r'[^\w\s]', '', title.lower()[:80])
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def is_duplicate(self, title):
        """ì¤‘ë³µ ì²´í¬"""
        title_hash = self.generate_hash(title)
        if title_hash in self.seen_titles:
            return True
        self.seen_titles.add(title_hash)
        return False
    
    def extract_domain(self, url):
        """ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            return domain.replace('www.', '')
        except:
            return 'Unknown'
    
    def calculate_importance(self, title, source, is_recent):
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 50
        
        # ìš°ì„  ì¶œì²˜
        if any(priority in source for priority in self.PRIORITY_SOURCES):
            score += 25
        
        # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        title_lower = title.lower()
        high_priority_words = ['billion', 'acquisition', 'launch', 'announces', 'breakthrough', 'major']
        score += sum(10 for word in high_priority_words if word in title_lower)
        
        # ìµœê·¼ì„±
        if is_recent:
            score += 15
        
        return min(score, 100)
    
    def categorize_article(self, title, snippet):
        """ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        text = f"{title} {snippet}".lower()
        
        if any(word in text for word in ['launch', 'release', 'unveil', 'announce']):
            return 'Product Launch'
        elif any(word in text for word in ['revenue', 'earnings', 'profit', 'stock']):
            return 'Financial Results'
        elif any(word in text for word in ['regulation', 'law', 'lawsuit', 'fine', 'court']):
            return 'Regulation & Policy'
        elif any(word in text for word in ['ai', 'artificial intelligence', 'technology', 'innovation']):
            return 'Technology Innovation'
        elif any(word in text for word in ['compete', 'rival', 'versus', 'market share']):
            return 'Competition'
        elif any(word in text for word in ['ceo', 'executive', 'appoint', 'resign']):
            return 'Leadership & Strategy'
        else:
            return 'Market Trends'
    
    def translate_to_korean(self, text):
        """ê°„ë‹¨í•œ í•œê¸€ ë²ˆì—­ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        translations = {
            'announces': 'ë°œí‘œ',
            'launch': 'ì¶œì‹œ',
            'acquires': 'ì¸ìˆ˜',
            'acquisition': 'ì¸ìˆ˜',
            'reports': 'ë³´ê³ ',
            'invests': 'íˆ¬ì',
            'investment': 'íˆ¬ì',
            'expands': 'í™•ì¥',
            'introduces': 'ë„ì…',
            'unveils': 'ê³µê°œ',
            'partners': 'íŒŒíŠ¸ë„ˆì‹­',
            'partnership': 'íŒŒíŠ¸ë„ˆì‹­ ì²´ê²°',
            'billion': 'ì‹­ì–µ',
            'million': 'ë°±ë§Œ',
            'AI': 'ì¸ê³µì§€ëŠ¥',
            'artificial intelligence': 'ì¸ê³µì§€ëŠ¥',
            'data center': 'ë°ì´í„°ì„¼í„°',
            'cloud': 'í´ë¼ìš°ë“œ',
            'revenue': 'ë§¤ì¶œ',
            'earnings': 'ì‹¤ì ',
            'profit': 'ì´ìµ',
            'stock': 'ì£¼ê°€',
            'CEO': 'CEO',
            'deal': 'ê±°ë˜',
            'agreement': 'ê³„ì•½'
        }
        
        result = text
        for eng, kor in translations.items():
            result = re.sub(r'\b' + re.escape(eng) + r'\b', f'{eng}({kor})', result, flags=re.IGNORECASE)
        
        return result
    
    def search_google_news(self, query, max_results=20):
        """Google News ê²€ìƒ‰"""
        try:
            # Google News RSS í”¼ë“œ ì‚¬ìš©
            encoded_query = quote(query)
            url = f'https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en'
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')[:max_results]
            
            articles = []
            for item in items:
                try:
                    title = item.title.text if item.title else ''
                    link = item.link.text if item.link else ''
                    pub_date = item.pubDate.text if item.pubDate else ''
                    description = item.description.text if item.description else ''
                    
                    # ë‚ ì§œ íŒŒì‹±
                    try:
                        pub_datetime = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                        date_str = pub_datetime.strftime('%Y-%m-%d')
                        is_recent = pub_datetime >= self.cutoff_date
                    except:
                        date_str = datetime.now().strftime('%Y-%m-%d')
                        is_recent = True
                    
                    # 14ì¼ í•„í„°
                    if not is_recent:
                        continue
                    
                    # ì¶œì²˜ ì¶”ì¶œ
                    source = self.extract_domain(link)
                    
                    articles.append({
                        'title': title,
                        'url': link,
                        'snippet': description,
                        'source': source,
                        'published_date': date_str,
                        'is_recent': is_recent
                    })
                    
                except Exception as e:
                    print(f"  âš ï¸  Item parsing error: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            print(f"  âŒ Search error for '{query}': {e}")
            return []
    
    def collect_news_for_company(self, company, keywords, target_count=50):
        """íŠ¹ì • ê¸°ì—…ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print(f"\nğŸ” Collecting news for {company}...")
        
        all_articles = []
        
        # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for keyword in keywords[:2]:  # ì£¼ìš” í‚¤ì›Œë“œ 2ê°œë§Œ ì‚¬ìš©
            search_query = f'{keyword} news'
            articles = self.search_google_news(search_query, max_results=30)
            
            for article in articles:
                # ì¤‘ë³µ ì²´í¬
                if self.is_duplicate(article['title']):
                    continue
                
                # íšŒì‚¬ëª… ê´€ë ¨ì„± ì²´í¬
                title_lower = article['title'].lower()
                if not any(kw.lower() in title_lower for kw in keywords):
                    continue
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                article['category'] = self.categorize_article(
                    article['title'], 
                    article['snippet']
                )
                
                # ì¤‘ìš”ë„ ì ìˆ˜
                article['importance_score'] = self.calculate_importance(
                    article['title'],
                    article['source'],
                    article['is_recent']
                )
                
                # í•œê¸€ ë²ˆì—­
                article['title_ko'] = self.translate_to_korean(article['title'])
                article['snippet_ko'] = self.translate_to_korean(article['snippet'])
                
                # íšŒì‚¬ ì •ë³´
                article['company'] = company
                
                all_articles.append(article)
                
                if len(all_articles) >= target_count:
                    break
            
            if len(all_articles) >= target_count:
                break
            
            time.sleep(1)  # Rate limiting
        
        # ì •ë ¬: ìš°ì„  ì¶œì²˜ > ì¤‘ìš”ë„ > ìµœì‹ ìˆœ
        all_articles.sort(key=lambda x: (
            -1 if any(p in x['source'] for p in self.PRIORITY_SOURCES) else 0,
            -x['importance_score'],
            x['published_date']
        ), reverse=True)
        
        print(f"  âœ… Found {len(all_articles)} articles")
        return all_articles[:100]  # ìµœëŒ€ 100ê°œ
    
    def collect_all_news(self):
        """ëª¨ë“  ê¸°ì—… ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print("\n" + "="*60)
        print("ğŸš€ BigTech News Crawler Started")
        print("="*60)
        
        for company, keywords in self.COMPANIES.items():
            articles = self.collect_news_for_company(company, keywords, target_count=50)
            self.news_data[company] = articles
            time.sleep(2)  # Rate limiting
        
        print("\n" + "="*60)
        print("âœ… Collection Completed!")
        print("="*60)
    
    def generate_json(self):
        """JSON ë°ì´í„° ìƒì„±"""
        stats = {}
        for company, articles in self.news_data.items():
            stats[f'{company}_count'] = len(articles)
        
        return {
            'timestamp': datetime.now().isoformat(),
            'statistics': stats,
            'news': self.news_data,
            'metadata': {
                'total_articles': sum(stats.values()),
                'companies': list(self.COMPANIES.keys()),
                'collection_date': datetime.now().strftime('%Y-%m-%d'),
                'cutoff_days': 14,
                'priority_sources': self.PRIORITY_SOURCES,
                'generated_by': 'Free BigTech News Crawler v1.0'
            }
        }
    
    def save_to_file(self, filename='bigtech_data_latest.json'):
        """JSON íŒŒì¼ ì €ì¥"""
        data = self.generate_json()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        total = data['metadata']['total_articles']
        print(f"\nğŸ’¾ Saved {total} articles to {filename}")
        
        # ê¸°ì—…ë³„ í†µê³„ ì¶œë ¥
        print("\nğŸ“Š Statistics:")
        for company, count in data['statistics'].items():
            print(f"  {company}: {count}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        crawler = BigTechNewsCrawler()
        crawler.collect_all_news()
        crawler.save_to_file()
        
        print("\nğŸ‰ Success! News data updated.")
        return 0
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
